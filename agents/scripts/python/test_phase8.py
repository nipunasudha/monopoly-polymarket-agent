"""
Phase 8 Integration Test Script

Tests hub status endpoints, WebSocket subscriptions, structured logging,
and performance metrics in a live environment.
"""
import asyncio
import os
import sys
from datetime import datetime

# Add agents to path
sys.path.insert(0, os.path.join(os.path.dirname(__file__), "../.."))

from agents.core.structured_logging import configure_structlog, get_logger, PerformanceMetrics
from agents.core.hub import TradingHub
from agents.core.session import Task, Lane
from agents.application.runner import AgentRunner


async def test_structured_logging():
    """Test 1: Structured logging configuration."""
    print("\n" + "=" * 60)
    print("TEST 1: Structured Logging")
    print("=" * 60)
    
    try:
        # Configure structured logging
        configure_structlog(level="INFO", json_output=False)
        logger = get_logger("test_phase8")
        
        # Test logging
        logger.info("phase8_test_started", test="structured_logging")
        logger.warning("test_warning", context="demo")
        
        print("‚úì Structured logging configured successfully")
        return True
    except Exception as e:
        print(f"‚úó Structured logging test failed: {e}")
        return False


async def test_performance_metrics():
    """Test 2: Performance metrics tracking."""
    print("\n" + "=" * 60)
    print("TEST 2: Performance Metrics")
    print("=" * 60)
    
    try:
        configure_structlog(level="INFO", json_output=False)
        logger = get_logger("test_metrics")
        metrics = PerformanceMetrics(logger)
        
        # Record various metrics
        metrics.record("test_value", 42)
        metrics.increment("counter", 5)
        metrics.increment("counter", 3)
        metrics.timing("operation_time", 123.45, operation="test")
        
        # Verify
        all_metrics = metrics.get_all()
        print(f"Recorded metrics: {all_metrics}")
        
        assert all_metrics["test_value"] == 42
        assert all_metrics["counter"] == 8
        
        print("‚úì Performance metrics working correctly")
        return True
    except Exception as e:
        print(f"‚úó Performance metrics test failed: {e}")
        return False


async def test_hub_with_metrics():
    """Test 3: Hub with performance metrics integration."""
    print("\n" + "=" * 60)
    print("TEST 3: Hub with Performance Metrics")
    print("=" * 60)
    
    try:
        configure_structlog(level="INFO", json_output=False)
        
        # Create hub (will use test_key, won't make real API calls)
        os.environ["ANTHROPIC_API_KEY"] = "test_key"
        hub = TradingHub()
        
        # Check if hub has metrics
        if hub.metrics:
            print("‚úì Hub initialized with performance metrics")
        else:
            print("‚ö† Hub initialized without metrics (structlog may not be available)")
        
        # Start hub
        await hub.start()
        
        # Enqueue some tasks
        for i in range(3):
            task = Task(
                id=f"test_task_{i}",
                prompt=f"Test task {i}",
                lane=Lane.RESEARCH,
                priority=10 - i
            )
            await hub.enqueue(task)
        
        # Get status
        status = hub.get_status()
        print(f"\nHub Status:")
        print(f"  Running: {status['running']}")
        print(f"  Sessions: {status['sessions']}")
        print(f"  Tasks Enqueued: {status['stats']['tasks_enqueued']}")
        
        if "metrics" in status:
            print(f"  Metrics: {status['metrics']}")
            print("‚úì Hub status includes performance metrics")
        else:
            print("‚ö† Hub status does not include metrics")
        
        await hub.stop()
        return True
    except Exception as e:
        print(f"‚úó Hub metrics test failed: {e}")
        import traceback
        traceback.print_exc()
        return False


async def test_runner_hub_status():
    """Test 4: AgentRunner hub status endpoints."""
    print("\n" + "=" * 60)
    print("TEST 4: AgentRunner Hub Status")
    print("=" * 60)
    
    try:
        # Test with new architecture
        os.environ["USE_NEW_ARCHITECTURE"] = "true"
        os.environ["ANTHROPIC_API_KEY"] = "test_key"
        
        runner = AgentRunner()
        
        try:
            status = runner.get_status()
        except Exception as db_error:
            # May fail if DB not initialized, just check architecture setup
            print(f"‚ö† Database error (expected in test): {str(db_error)[:100]}")
            print("‚úì Runner initialized with new architecture (DB validation skipped)")
            return True
        
        print(f"\nRunner Status (New Architecture):")
        print(f"  Architecture: {status['architecture']}")
        print(f"  State: {status['state']}")
        
        if "hub_status" in status:
            hub_status = status["hub_status"]
            print(f"  Hub Running: {hub_status.get('running', 'N/A')}")
            print(f"  Hub Sessions: {hub_status.get('sessions', 'N/A')}")
            print(f"  Hub Stats: {hub_status.get('stats', {})}")
            print("‚úì Hub status available in runner")
        else:
            print("‚úó Hub status not available in runner")
            return False
        
        # Test with legacy architecture
        os.environ["USE_NEW_ARCHITECTURE"] = "false"
        runner_legacy = AgentRunner()
        
        try:
            status_legacy = runner_legacy.get_status()
        except Exception:
            print("‚úì Legacy runner initialized (DB validation skipped)")
            return True
        
        print(f"\nRunner Status (Legacy Architecture):")
        print(f"  Architecture: {status_legacy['architecture']}")
        
        if "hub_status" not in status_legacy:
            print("‚úì Hub status correctly not available in legacy mode")
        else:
            print("‚ö† Hub status unexpectedly available in legacy mode")
        
        return True
    except Exception as e:
        print(f"‚úó Runner hub status test failed: {e}")
        import traceback
        traceback.print_exc()
        return False


async def test_hub_status_with_lanes():
    """Test 5: Hub status includes lane information."""
    print("\n" + "=" * 60)
    print("TEST 5: Hub Status with Lane Information")
    print("=" * 60)
    
    try:
        os.environ["ANTHROPIC_API_KEY"] = "test_key"
        configure_structlog(level="INFO", json_output=False)
        
        hub = TradingHub()
        await hub.start()
        
        # Enqueue tasks in different lanes
        lanes_to_test = [Lane.MAIN, Lane.RESEARCH, Lane.MONITOR, Lane.CRON]
        
        for i, lane in enumerate(lanes_to_test):
            task = Task(
                id=f"lane_test_{lane.value}_{i}",
                prompt=f"Test task for {lane.value}",
                lane=lane,
                priority=5
            )
            await hub.enqueue(task)
        
        # Get status
        status = hub.get_status()
        lane_status = status.get("lane_status", {})
        
        print(f"\nLane Status:")
        for lane_name, info in lane_status.items():
            print(f"  {lane_name}:")
            print(f"    Queued: {info['queued']}")
            print(f"    Active: {info['active']}")
            print(f"    Limit: {info['limit']}")
        
        # Verify all lanes are present (lowercase keys)
        assert "main" in lane_status or "MAIN" in lane_status
        assert "research" in lane_status or "RESEARCH" in lane_status
        assert "monitor" in lane_status or "MONITOR" in lane_status
        assert "cron" in lane_status or "CRON" in lane_status
        
        print("\n‚úì Hub status includes all lane information")
        
        await hub.stop()
        return True
    except Exception as e:
        print(f"‚úó Lane status test failed: {e}")
        import traceback
        traceback.print_exc()
        return False


async def test_metrics_over_time():
    """Test 6: Metrics tracking over multiple operations."""
    print("\n" + "=" * 60)
    print("TEST 6: Metrics Tracking Over Time")
    print("=" * 60)
    
    try:
        os.environ["ANTHROPIC_API_KEY"] = "test_key"
        configure_structlog(level="INFO", json_output=False)
        
        hub = TradingHub()
        await hub.start()
        
        # Simulate multiple enqueues
        for i in range(5):
            task = Task(
                id=f"metrics_test_{i}",
                prompt=f"Metrics test task {i}",
                lane=Lane.RESEARCH,
                priority=i
            )
            await hub.enqueue(task)
            await asyncio.sleep(0.1)
        
        # Check stats
        status = hub.get_status()
        stats = status["stats"]
        
        print(f"\nStats after 5 enqueues:")
        print(f"  Tasks Enqueued: {stats['tasks_enqueued']}")
        print(f"  Tasks Completed: {stats['tasks_completed']}")
        print(f"  Tasks Failed: {stats['tasks_failed']}")
        
        if hub.metrics:
            metrics = hub.metrics.get_all()
            print(f"\nPerformance Metrics:")
            for key, value in metrics.items():
                print(f"  {key}: {value}")
        
        assert stats["tasks_enqueued"] == 5
        print("\n‚úì Metrics tracked correctly over time")
        
        await hub.stop()
        return True
    except Exception as e:
        print(f"‚úó Metrics over time test failed: {e}")
        import traceback
        traceback.print_exc()
        return False


async def main():
    """Run all Phase 8 integration tests."""
    print("=" * 60)
    print("PHASE 8 INTEGRATION TESTS")
    print("Observability & WebSocket Status Updates")
    print("=" * 60)
    
    results = []
    
    # Run all tests
    results.append(("Structured Logging", await test_structured_logging()))
    results.append(("Performance Metrics", await test_performance_metrics()))
    results.append(("Hub with Metrics", await test_hub_with_metrics()))
    results.append(("Runner Hub Status", await test_runner_hub_status()))
    results.append(("Hub Status with Lanes", await test_hub_status_with_lanes()))
    results.append(("Metrics Over Time", await test_metrics_over_time()))
    
    # Summary
    print("\n" + "=" * 60)
    print("TEST SUMMARY")
    print("=" * 60)
    
    passed = sum(1 for _, result in results if result)
    total = len(results)
    
    for name, result in results:
        status = "‚úì PASS" if result else "‚úó FAIL"
        print(f"{status}: {name}")
    
    print(f"\nTotal: {passed}/{total} tests passed")
    
    if passed == total:
        print("\nüéâ All Phase 8 tests passed!")
        return 0
    else:
        print(f"\n‚ö†Ô∏è {total - passed} test(s) failed")
        return 1


if __name__ == "__main__":
    exit_code = asyncio.run(main())
    sys.exit(exit_code)
